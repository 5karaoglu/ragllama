llama-index==0.10.12
llama-index-llms-huggingface==0.1.4
llama-index-embeddings-huggingface==0.2.0
llama-index-readers-json==0.1.3
llama-index-llms-vllm==0.1.3
sqlite-utils==3.35.2
transformers==4.38.0
torch==2.1.2
accelerate==0.25.0
bitsandbytes==0.41.3
sentence-transformers==2.6.1
python-dotenv==1.0.0
tqdm==4.66.1
colorlog==6.7.0
nvidia-ml-py3==7.352.0
flask==2.3.3
gunicorn==21.2.0
PyPDF2==3.0.1
safetensors==0.4.1
einops==0.7.0
optimum==1.16.1
protobuf==4.24.4
peft==0.6.2
vllm==0.3.2
# KV Cache quantization için gerekli paketler
quanto==0.0.9  # torch 2.1.2 ile uyumlu olan sürüm
hqq  # KV Cache niceleme için alternatif backend
# Aşağıdaki paketler GPU hızlandırma için opsiyonel
# Kurulum hatası alınırsa yorum satırına alınabilir
# xformers==0.0.22.post7
# triton==2.1.0
# Flash Attention KV Cache quantization ile uyumludur
# Kurulum için: FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn==2.4.2 --no-build-isolation
flash-attn==2.4.2 