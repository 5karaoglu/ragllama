version: '3.8'

services:
  rag-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    volumes:
      # Veri dosyasını bağla
      - ./Book1.json:/app/Book1.json
      - ./document.pdf:/app/document.pdf
      # Model ve indeks önbelleklerini kalıcı hale getir
      - model_cache:/app/model_cache
      - embedding_cache:/app/embedding_cache
      - storage:/app/storage
      - pdf_storage:/app/pdf_storage
    # Shared memory boyutunu artır (RTX 4090 için yeterli olacak)
    shm_size: 16G
    environment:
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0,1  # İki GPU'yu da görünür yap
      - LLM_TYPE=vllm  # vLLM kullanımını etkinleştir (seçenek: huggingface)
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,garbage_collection_threshold:0.8  # CUDA bellek ayarlarını optimize et
      - TRANSFORMERS_CACHE=/app/model_cache  # Transformers önbelleğini ayarla
      - HF_HOME=/app/model_cache  # HuggingFace önbelleğini ayarla
      - VLLM_USE_PAGED_ATTENTION=true  # vLLM PagedAttention etkinleştir
      - VLLM_ATTENTION_SHARD_SIZE=1024  # vLLM için dikkat mekanizması parça boyutu
      - VLLM_MAX_PARALLEL_LOADING_WORKERS=2  # Model yükleme için paralel işçi sayısı
      - VLLM_GPU_MEMORY_UTILIZATION=0.60  # GPU bellek kullanım oranını azalt (0.75 -> 0.60)
      - VLLM_TENSOR_PARALLEL_SIZE=2  # 2 GPU üzerinde tensor paralelizmi
      - BITSANDBYTES_NOWELCOME=1  # BitsAndBytes hoş geldin mesajını gizle
      - HF_HUB_ENABLE_HF_TRANSFER=1  # Hub'dan daha hızlı model indirme
      - KV_CACHE_BACKEND=quanto  # KV Cache için quanto backend kullan
      - KV_CACHE_NBITS=4  # KV Cache için 4-bit quantization
      - KV_CACHE_RESIDUAL_LENGTH=128  # Genelde optimal değer
      - OMP_NUM_THREADS=8  # CPU iş parçacığı sayısını sınırla
      - TOKENIZERS_PARALLELISM=true  # Tokenizer paralelliğini etkinleştir
      # Ray için bellek limiti ve yapılandırması
      - RAY_memory_monitor_refresh_ms=0  # Bellek izlemeyi devre dışı bırak
      - RAY_object_store_memory=20000000000  # ~20GB - Ray için nesne deposu boyutu
      # GPU önbelleğini temizle
      - CUDA_CACHE_DISABLE=0  # CUDA önbelleğini etkinleştir
      # vLLM disk önbelleğini etkinleştir
      - VLLM_ENABLE_DISK_CACHE=true
      # Flash Attention 2 etkinleştir
      - USE_FLASH_ATTENTION=true
    restart: unless-stopped
    runtime: nvidia  # NVIDIA runtime'ı açıkça belirt
    # GPU desteği ve kaynak sınırlamaları
    deploy:
      resources:
        limits:
          memory: 64G  # Bellek sınırını artır (48G -> 64G)
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']  # İlk iki GPU'yu kullan (indeks 0 ve 1)
              capabilities: [gpu, utility, compute]  # Tüm GPU yeteneklerini etkinleştir
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/status"]
      interval: 60s  # Kontrol aralığını artır
      timeout: 20s  # Zaman aşımını artır
      retries: 3
      start_period: 300s  # Başlangıç süresini artır (büyük model yükleme süresi için)
    # Ray ve vLLM için ek tmpfs bağlaması
    tmpfs:
      - /dev/shm

volumes:
  model_cache:
  embedding_cache:
  storage:
  pdf_storage: 